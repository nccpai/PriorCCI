{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da45774e-8a87-4171-8354-46b174df4888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 13:32:59.335922: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-18 13:32:59.338667: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-18 13:32:59.434952: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-18 13:32:59.809053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 13:33:00.832152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7934031-7a45-4d3f-ace7-2602ce567fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e496cf-4ebf-445e-aaf8-1c9c4143e6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['combi-cells_c0.npz',\n",
       " 'combi-cells_c1.npz',\n",
       " 'combi-cells_c2.npz',\n",
       " 'combi-cells_c3.npz',\n",
       " 'combi-cells_c4.npz',\n",
       " 'combi-cells_c5.npz',\n",
       " 'combi-cells_c6.npz',\n",
       " 'combi-cells_c7.npz',\n",
       " 'combi-cells_c8.npz',\n",
       " 'combi-cells_c9.npz',\n",
       " 'combi-cells_c10.npz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'data/'\n",
    "\n",
    "# Function to extract numbers from filenames\n",
    "def extract_number(filename):\n",
    "    return int(re.search(r'\\d+', filename).group())\n",
    "\n",
    "class_files = sorted([f for f in os.listdir(path) if f.endswith(\".npz\")], key=extract_number)\n",
    "class_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3abe48c-f626-4ba7-8cdd-b42282fa37bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading done:  0\n",
      "Loading done:  1\n",
      "Loading done:  2\n",
      "Loading done:  3\n",
      "Loading done:  4\n",
      "Loading done:  5\n",
      "Loading done:  6\n",
      "Loading done:  7\n",
      "Loading done:  8\n",
      "Loading done:  9\n",
      "Loading done:  10\n",
      "Total data shape: (5500, 100, 13048, 2)\n",
      "Total labels shape: (5500,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists for data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load data and assign labels for each class file\n",
    "for i, class_file in enumerate(class_files):\n",
    "    loaded = np.load(path + class_file)\n",
    "    class_data = np.stack([loaded[key] for key in loaded.files])\n",
    "    class_labels = np.ones(class_data.shape[0]) * i\n",
    "    \n",
    "    data.append(class_data)\n",
    "    labels.append(class_labels)\n",
    "\n",
    "    \n",
    "    print(\"Loading done: \", i)\n",
    "\n",
    "# Concatenate data and labels\n",
    "data = np.concatenate(data, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "data[np.isnan(data)] = 0\n",
    "\n",
    "print(f\"Total data shape: {data.shape}\")\n",
    "print(f\"Total labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e6094e-ab6f-4ac9-8866-f261c4320273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13048 11\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_tmp, test_data, Y_tmp, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(X_tmp, Y_tmp, test_size=0.2, random_state=44)\n",
    "\n",
    "num_lrpair = data.shape[2]\n",
    "num_classes = len(class_files)\n",
    "print(num_lrpair, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42dc5a7f-2fc6-4d10-bc20-aaabd3e9f5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanbyeol/anaconda3/envs/hanbyeol/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-07-18 13:47:11.284501: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Create the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    # First Conv2D layer for reducing channels\n",
    "    layers.Conv2D(8, (1, 1), activation='relu', kernel_initializer='he_normal', \n",
    "                  input_shape=(100, num_lrpair, 2)),\n",
    "    # Second Conv2D layer\n",
    "    layers.Conv2D(16, (10, 1), strides=(10, 1), activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    # Additional Conv2D layers or other layers\n",
    "    layers.Conv2D(16, (10, 1), strides=(10, 1), activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.MaxPooling2D((1, 4)),\n",
    "    layers.Conv2D(32, (1, 4), activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((1, 2)),\n",
    "    # Global Average Pooling layer\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    # Fully connected layers\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy', f1_m, precision_m, recall_m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b52aa-7b8f-40d3-baf4-2ca8c9e0d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_data, train_labels, \n",
    "                    validation_data=(val_data, val_labels), \n",
    "                    epochs=12, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962df949-4e2c-4e94-ba38-5155b88d703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_data, test_labels, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979bf38-75ca-455f-9ce0-44f7add6b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "save_path = \"cnn_model/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "model.save(save_path + 'data_cnn-model_v01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de7b09-bb91-4bf4-a2e7-7164bbc696a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aacef1-d5b5-4acc-8338-720fbfd097df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7ff13af-f977-456f-985b-ef9f15c21799",
   "metadata": {},
   "source": [
    "## GradCAM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e66caf-38f8-442e-982d-1873bc8186a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_keras_vis.gradcam import GradcamPlusPlus\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "\n",
    "# load data\n",
    "model_paths = \"cnn_model/\"\n",
    "model_names = sorted([f for f in os.listdir(model_paths) if f.endswith('.h5')])\n",
    "\n",
    "class_names = ['Tip_Cells', 'activated capillary', 'Immature_Phenotype',\n",
    "               'capillary_I', 'capillary_II', 'Activated_EC', 'TandNK', \n",
    "               'Epithelial', 'Myeloid', 'Fibro_Peri', 'B']\n",
    "\n",
    "# Load L-R interaction gene data\n",
    "file_path = 'DB/'\n",
    "gene_list_df = pd.read_csv(file_path + 'CCIdb.csv')\n",
    "genes = gene_list_df['TumorGene'].tolist()\n",
    "genes1 = gene_list_df['OtherGene'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d452ddf-1fb0-415c-9db4-ac4d901b0bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: gcamplus_result_Tip_Cells_v08.txt\n",
      "Completed: gcamplus_result_activated capillary_v08.txt\n",
      "Completed: gcamplus_result_Immature_Phenotype_v08.txt\n",
      "Completed: gcamplus_result_capillary_I_v08.txt\n",
      "Completed: gcamplus_result_capillary_II_v08.txt\n",
      "Completed: gcamplus_result_Activated_EC_v08.txt\n",
      "Completed: gcamplus_result_TandNK_v08.txt\n",
      "Completed: gcamplus_result_Epithelial_v08.txt\n",
      "Completed: gcamplus_result_Myeloid_v08.txt\n",
      "Completed: gcamplus_result_Fibro_Peri_v08.txt\n",
      "Completed: gcamplus_result_B_v08.txt\n"
     ]
    }
   ],
   "source": [
    "# Ensure the save path exists\n",
    "save_path = 'res/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "for model_name in model_names:\n",
    "    version_suffix = model_name.split('_')[-1].replace('.h5', '')\n",
    "    model = tf.keras.models.load_model(model_paths + model_name,\n",
    "                                  custom_objects={'f1_m':f1_m, \n",
    "                                                  'precision_m':precision_m, \n",
    "                                                  'recall_m':recall_m})\n",
    "\n",
    "   # Create GradCAM++ object\n",
    "    gradcam = GradcamPlusPlus(model, model_modifier=ReplaceToLinear(), clone=True)\n",
    "\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        # Select data for the class\n",
    "        start_index = class_index * 500  \n",
    "        end_index = start_index + 500\n",
    "        class_data = data[start_index:end_index]\n",
    "        class_labels = np.full((500,), class_index)  \n",
    "        \n",
    "        # Generate heatmap\n",
    "        cam_tot = np.mean([\n",
    "            gradcam(CategoricalScore(label), np.expand_dims(sample, axis=0), penultimate_layer=-1)\n",
    "            for label, sample in zip(class_labels, class_data)\n",
    "        ], axis=0)\n",
    "        \n",
    "        # Normalize cam_tot\n",
    "        cam_sum = np.mean(cam_tot[0], axis=0)\n",
    "        cam_max, cam_min = cam_sum.max(), cam_sum.min()\n",
    "        cam_mod = (cam_sum - cam_min) / (cam_max - cam_min)\n",
    "        \n",
    "        # Save results to file\n",
    "        line_1st = f'TumorCell\\t{class_name}\\tNormalized_Weight'\n",
    "        line_ext = [f'{genes[i]}\\t{genes1[i]}\\t{str(cam_mod[i])}' for i in range(len(genes))]\n",
    "        \n",
    "        file_name = f\"gcamplus_result_{class_name}_{version_suffix}.txt\"\n",
    "        full_path = os.path.join(save_path, file_name)\n",
    "        with open(full_path, \"w\") as f_out:\n",
    "            f_out.write(line_1st + '\\n')\n",
    "            f_out.write('\\n'.join(line_ext))\n",
    "        \n",
    "        print(f\"Completed: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e1b3d-8bc3-483d-8e6d-c052a8f9dc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eeeee7-b958-4ec2-9d7c-72b16a2b971f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
