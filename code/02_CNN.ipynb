{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da45774e-8a87-4171-8354-46b174df4888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 17:58:43.386904: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-09 17:58:43.388765: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-09 17:58:43.394562: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-09 17:58:43.409866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749459523.435165 1089384 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749459523.442415 1089384 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-09 17:58:43.470461: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_number(filename):\n",
    "    return int(re.search(r'\\d+', filename).group())\n",
    "\n",
    "def load_data(path='cnn_input_data/'):\n",
    "    class_files = sorted([f for f in os.listdir(path) if f.endswith(\".npz\")], key=extract_number)\n",
    "    \n",
    "    data, labels = [], []\n",
    "    for i, class_file in enumerate(class_files):\n",
    "        loaded = np.load(os.path.join(path, class_file))\n",
    "        class_data = np.stack([loaded[k] for k in loaded.files])\n",
    "        class_labels = np.ones(class_data.shape[0]) * i\n",
    "\n",
    "        data.append(class_data)\n",
    "        labels.append(class_labels)\n",
    "        print(\"Loaded:\", class_file)\n",
    "\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    data[np.isnan(data)] = 0  # NaN 제거\n",
    "    print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")\n",
    "    return data, labels, len(class_files), data.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7812638-293b-4ac4-88ff-1e4097e8ca7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b93a9d-a3fd-444b-bc19-9895c01a722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    y_true = K.one_hot(K.cast(y_true, 'int32'), num_classes)\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    pred_pos = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return tp / (pred_pos + K.epsilon())\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    y_true = K.one_hot(K.cast(y_true, 'int32'), num_classes)\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    actual_pos = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return tp / (actual_pos + K.epsilon())\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    p = precision_m(y_true, y_pred)\n",
    "    r = recall_m(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Conv2D(8, (1, 1), activation='relu', kernel_initializer='he_normal', input_shape=input_shape),\n",
    "        layers.Conv2D(16, (10, 1), strides=(10, 1), activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(16, (10, 1), strides=(10, 1), activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.MaxPooling2D((1, 4)),\n",
    "        layers.Conv2D(32, (1, 4), activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((1, 2)),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def train_and_save_model(data, labels, num_classes, num_lrpair,\n",
    "                         base_save_path='cnn_model/', n_repeat=10, batch_size=32, epochs=80):\n",
    "    os.makedirs(base_save_path, exist_ok=True)\n",
    "\n",
    "    for i in range(1, n_repeat + 1):\n",
    "        print(f\"▶ Training model {i}/{n_repeat}\")\n",
    "        rs = 41 + i\n",
    "        X_tmp, test_data, Y_tmp, test_labels = train_test_split(data, labels, test_size=0.2, random_state=rs)\n",
    "        train_data, val_data, train_labels, val_labels = train_test_split(X_tmp, Y_tmp, test_size=0.2, random_state=rs + 1)\n",
    "\n",
    "        model = create_model((100, num_lrpair, 2), num_classes)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy', f1_m, precision_m, recall_m])\n",
    "\n",
    "        lr_reduction = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-6)\n",
    "\n",
    "        model.fit(train_data, train_labels, validation_data=(val_data, val_labels),\n",
    "                  epochs=epochs, batch_size=batch_size, callbacks=[lr_reduction], verbose=1)\n",
    "\n",
    "        model.save(os.path.join(base_save_path, f\"data_cnn-model_v0{i}.h5\"))\n",
    "        print(f\"✔ Model {i} saved.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c15ac-c9e0-491d-965a-5a6785dc4d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4096f61-3cb4-409f-a18b-a7272a3d7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_saved_models(data, labels, num_classes, model_dir='cnn_model/', n_repeat=10):\n",
    "    for i in range(1, n_repeat + 1):\n",
    "        print(f\"=== Model v{i} Evaluation ===\")\n",
    "        rs = 41 + i\n",
    "        _, test_data, _, test_labels = train_test_split(data, labels, test_size=0.2, random_state=rs)\n",
    "\n",
    "        model_path = os.path.join(model_dir, f\"data_cnn-model_v0{i}.h5\")\n",
    "        model = load_model(model_path, custom_objects={\n",
    "            'f1_m': f1_m,\n",
    "            'precision_m': precision_m,\n",
    "            'recall_m': recall_m\n",
    "        })\n",
    "\n",
    "        y_pred_probs = model.predict(test_data, batch_size=32, verbose=0)\n",
    "        pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "        true_labels = test_labels.astype(int)\n",
    "\n",
    "        f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
    "        recall = recall_score(true_labels, pred_labels, average='weighted')\n",
    "        precision = precision_score(true_labels, pred_labels, average='weighted')\n",
    "        loss, accuracy, *_ = model.evaluate(test_data, true_labels, verbose=0)\n",
    "\n",
    "        print(f\"Loss: {loss:.4f} | Accuracy: {accuracy:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037452c-4a1b-4c71-9f23-dff09075f203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28d8bd-1938-4394-9f15-8d84ce8d4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "def visualize_final_model_results(model, test_data, test_labels, class_names=None):\n",
    "    \"\"\"\n",
    "    Visualize confusion matrix and ROC-AUC curves for a trained multi-class classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.Model\n",
    "        Trained Keras model.\n",
    "    test_data : np.ndarray\n",
    "        Input features for test set.\n",
    "    test_labels : np.ndarray\n",
    "        True labels for test set (integer class labels).\n",
    "    class_names : list of str, optional\n",
    "        Class name labels. If None, use integer indices.\n",
    "    \"\"\"\n",
    "    # === Confusion Matrix ===\n",
    "    predictions = model.predict(test_data)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(test_labels, predicted_classes)\n",
    "    n_classes = len(np.unique(test_labels))\n",
    "    \n",
    "    if class_names is None:\n",
    "        class_names = [f'Class {i}' for i in range(n_classes)]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    # === ROC-AUC Curve ===\n",
    "    y_test_bin = label_binarize(test_labels, classes=np.arange(n_classes))\n",
    "    y_score = predictions\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['blue', 'red', 'green', 'cyan', 'magenta', 'yellow', \n",
    "                    'black', 'orange', 'purple', 'brown', 'gray'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'{class_names[i]} (AUC = {roc_auc[i]:0.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('Multi-class ROC curve')\n",
    "    plt.legend(loc='lower right', fontsize=8)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d8b7f-e731-408d-9adf-bcc2687614cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e24063-adb5-4cbd-8882-a446ba149443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers, Model\n",
    "from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "\n",
    "def convert_sequential_to_functional(model):\n",
    "    inputs = layers.Input(shape=model.input_shape[1:])\n",
    "    x = inputs\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "    return Model(inputs, x)\n",
    "\n",
    "def run_gradcam_analysis(data, gene_list_csv, model_dir='cnn_model/', \n",
    "                         save_dir='gcam_res/', class_files=None, \n",
    "                         custom_objects=None, data_points=1000):\n",
    "    \"\"\"\n",
    "    Apply GradCAM++ to all trained models and generate gene importance files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        CNN input data of shape (n_samples, height, width, 2).\n",
    "    gene_list_csv : str\n",
    "        Path to filtered CCIdb CSV containing columns 'A', 'B'.\n",
    "    model_dir : str\n",
    "        Directory containing saved CNN models (.h5).\n",
    "    save_dir : str\n",
    "        Output directory to store GradCAM result files.\n",
    "    class_files : list of str\n",
    "        List of .npz class input file names used to determine class labels.\n",
    "    custom_objects : dict\n",
    "        Custom metrics used in model loading.\n",
    "    data_points : int\n",
    "        Number of data points per class (default 1000).\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    model_names = sorted([f for f in os.listdir(model_dir) if f.endswith('.h5')])\n",
    "    class_names = [re.search(r'combi-(.*)_c\\d+\\.npz', file).group(1) for file in class_files]\n",
    "\n",
    "    gene_df = pd.read_csv(gene_list_csv)\n",
    "    genes_A = gene_df['A'].tolist()\n",
    "    genes_B = gene_df['B'].tolist()\n",
    "\n",
    "    for model_name in model_names:\n",
    "        version_suffix = model_name.split('_')[-1].replace('.h5', '')\n",
    "        model_path = os.path.join(model_dir, model_name)\n",
    "        print(f\"🔍 Loading model: {model_path}\")\n",
    "\n",
    "        model = load_model(model_path, custom_objects=custom_objects)\n",
    "        model = convert_sequential_to_functional(model)\n",
    "        gradcam = GradcamPlusPlus(model, model_modifier=ReplaceToLinear(), clone=True)\n",
    "\n",
    "        for class_index, class_name in enumerate(class_names):\n",
    "            start = class_index * data_points\n",
    "            end = start + data_points\n",
    "            class_data = data[start:end]\n",
    "            class_labels = np.full((data_points,), class_index)\n",
    "\n",
    "            cam_tot = np.mean([\n",
    "                gradcam(CategoricalScore(label), np.expand_dims(sample, axis=0), penultimate_layer=-1)\n",
    "                for label, sample in zip(class_labels, class_data)\n",
    "            ], axis=0)\n",
    "\n",
    "            cam_mean = np.mean(cam_tot[0], axis=0)\n",
    "            cam_norm = (cam_mean - cam_mean.min()) / (cam_mean.max() - cam_mean.min())\n",
    "\n",
    "            output_txt = os.path.join(save_dir, f\"gcamplus_result_{class_name}_{version_suffix}.txt\")\n",
    "            with open(output_txt, 'w') as f_out:\n",
    "                f_out.write(f\"TumorCell\\t{class_name}\\tNormalized_Weight\\n\")\n",
    "                for a, b, w in zip(genes_A, genes_B, cam_norm):\n",
    "                    f_out.write(f\"{a}\\t{b}\\t{w}\\n\")\n",
    "\n",
    "            print(f\"✅ Completed: {output_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53b0cc-52a6-408e-9fa6-01534d68629c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce60eb-9b3b-434b-8bf5-2bdd0a4d6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "data, labels, num_classes, num_lrpair = load_data(path='cnn_input_data/')\n",
    "\n",
    "# 학습\n",
    "train_and_save_model(data, labels, num_classes, num_lrpair)\n",
    "\n",
    "# 평가\n",
    "evaluate_saved_models(data, labels, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c3f39-a8fa-48f4-981d-2ed4cfa35a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59723993-50dd-44b4-a065-b824acebca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 마지막 모델 불러오기\n",
    "model_path = 'cnn_model/data_cnn-model_v010.h5'\n",
    "model = load_model(model_path, custom_objects={\n",
    "    'f1_m': f1_m,\n",
    "    'precision_m': precision_m,\n",
    "    'recall_m': recall_m\n",
    "})\n",
    "\n",
    "# 데이터 분할 (test set만)\n",
    "_, test_data, _, test_labels = train_test_split(data, labels, test_size=0.2, random_state=51)\n",
    "\n",
    "# 시각화 실행\n",
    "visualize_final_model_results(model, test_data, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ae756-3f68-4a10-8421-ca4b26c657c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e19fc9-bccd-4757-85a6-0a3a19fa42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradcam_module import run_gradcam_analysis\n",
    "\n",
    "# npz로부터 로딩한 class_files와 data를 사용한다고 가정\n",
    "custom_objs = {'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m}\n",
    "run_gradcam_analysis(data=data,\n",
    "                     gene_list_csv='DB/filtered_CCIdb.csv',\n",
    "                     model_dir='cnn_model/',\n",
    "                     save_dir='gcam_res/',\n",
    "                     class_files=class_files,\n",
    "                     custom_objects=custom_objs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
